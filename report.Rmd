---
title: "Final Project"
author: "London Wagner"
date: "4/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r message=FALSE}
# library(here)
library(leaps)
library(tree)
library(glmnet)
library(tidyverse)
```

## Loading in data

```{r message=FALSE}
student_por <- read_csv2("data/student-por.csv")
```


## Introduction

For our final project we are looking at the student performance data set from the UCI Machine Learning Repository that measures student performance in a Portuguese class based on a variety of predictors. There predictors include but are not limited to:

- student sex
- student age
- previous grades
- access to internet, etc

The person who originally collected this data and performed the study was Paulo Cortez from the University of Minho. 

The purpose of our analysis is determine which variables are significant when predicting a student's final grade in the Portuguese class.

Here is our plan for analyzing the data

1) Run a linear regression model with final grade as the response and all other variables as predictors 
2) Use best subset, forward step, and backwards step to select variables for a reduced model
3) Use ridge and lasso
4) Use cross validation methods to determine which model predicts final grade with the greatest accuracy
5) Create a tree using the variables from the best model

## Cleaning Data

```{r}
student_por <-
  student_por %>%
  mutate(school = factor(school),
         sex = factor(sex),
         address = factor(address),
         famsize = factor(famsize),
         Pstatus = factor(Pstatus),
         schoolsup = factor(schoolsup),
         famsup = factor(famsup),
         paid = factor(paid),
         activities = factor(activities),
         nursery = factor(nursery),
         higher = factor(higher),
         internet = factor(internet),
         romantic = factor(romantic))

head(student_por)
```

## Running a simple linear regression

```{r}
por_reg <- lm(G3 ~ ., data = student_por)
summary(por_reg)
```

**We should write out the model here**

## Set validation on initial regression
```{r}
n <- length(student_por)

Z <- sample(n, n / 2)

reg.fit <- lm(G3 ~ ., data = student_por[Z])
```

```{r}
g3_predicted <- predict(reg.fit, student_por)
```

```{r}
plot(student_por$G3[-Z], g3_predicted[-Z], xlab = "Actual Grades", ylab = "Predicted Grades", main = "Prediction Accuracy of Full Linear Model")
abline(0,1)
```

```{r}
mean((student_por$G3 - g3_predicted)[-Z] ^ 2)
```

A lot of the predictors do not appear to be significant, so we are going to use some variable selection methods to simplify the model while still maintaining accuracy.

## Best Subset

```{r}
subsets <- regsubsets(G3 ~ ., data = student_por, nvmax = 15)
summary(subsets)
summary(subsets)$adjr2
summary(subsets)$cp
summary(subsets)$bic
```

## Set validation for Best Subset
## Best Subset
```{r}
which.min(summary(subsets)$adjr2)
which.min(summary(subsets)$cp)
which.min(summary(subsets)$bic)
```

## Model Based on Adjusted R-squared
```{r}
set.seed(1)
Z <- sample(n, n / 2)

reg.bestsubR <- lm(G3 ~ G2, data = student_por, subset = Z)

g3_pred_bestsubR <- predict(reg.bestsubR, student_por)
```

```{r}
plot(student_por$G3[-Z], g3_pred_bestsubR[-Z], xlab = "Actual Grades", ylab = "Predicted Grades", main = "Prediction Accuracy of Reduced Model 1")
abline(0,1)
```

```{r}
mean((student_por$G3 - g3_pred_bestsubR)[-Z] ^ 2)
```

## Model Based on Mallow's Cp
**ran into error with levels?**

```{r}
# ACTUAL MODEL
# reg.bestsubCP <- lm(G3 ~ sex + address + Mjob + reason + guardian + traveltime + failures + Dalc + absences + G1 + G2,
#                     data = student_por, subset = Z)

reg.bestsubCP <- lm(G3 ~ sex + address  + reason + guardian + traveltime + failures + Dalc + absences + G1 + G2,
                    data = student_por, subset = Z)

g3_pred_bestsubCP <- predict(reg.bestsubCP, student_por)
```

```{r}
plot(student_por$G3[-Z], g3_pred_bestsubCP[-Z], xlab = "Actual Grades", ylab = "Predicted Grades", main = "Prediction Accuracy of Reduced Model 2")
abline(0,1)
```

```{r}
mean((student_por$G3 - g3_pred_bestsubCP)[-Z] ^ 2)
```

## Model Based on BIC
```{r}
reg.bestsubBIC <- lm(G3 ~ reason + G1 + G2, data = student_por, subset = Z)

g3_pred_bestsubBIC <- predict(reg.bestsubBIC, student_por)
```

```{r}
plot(student_por$G3[-Z], g3_pred_bestsubBIC[-Z], xlab = "Actual Grades", ylab = "Predicted Grades", main = "Prediction Accuracy of Reduced Model 3")
abline(0,1)
```

```{r}
mean((student_por$G3 - g3_pred_bestsubBIC)[-Z] ^ 2)
```


## Forward Step

```{r}
null <- lm(G3 ~ 1, data = student_por)
full <- lm(G3 ~ ., data = student_por)

forward <- step(null, scope = list(lower=null, upper = full), direction = "forward")

summary(forward)
```

```{r}
backward <- step(full, scope = list(lower=null, upper = full), direction = "backward")

summary(backward)
```

## Set validation
```{r}
# RUNNING INTO ERROR: Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
  # contrasts can be applied only to factors with 2 or more levels
# reg.forward <- lm(G3 ~ G2 + G1 + failures + reason + absences + sex + school + traveltime + health, data = student_por, subset = Z)

g3_pred_forward <- predict(reg.forward, student_por)
```

```{r}
plot(student_por$G3[-Z], g3_pred_forward[-Z])
abline(0, 1)
```

```{r}
mean((student_por$G3 - g3_pred_forward)[-Z] ^ 2)
```

## Ridge Regression & LASSO Preparation

```{r}
# Training/test split
set.seed(1)
train <- sample(1:nrow(student_por), .7*nrow(student_por))
G3_test <- student_por$G3[-train]

# Creating model matrix for rr and lasso calculations
x_col <- model.matrix(G3 ~ ., student_por)[, -1]
```

## Ridge Regression

```{r}
set.seed(1)
cv.out1 <- cv.glmnet(x_col, student_por$G3, alpha = 0)
predict(cv.out1, s = cv.out1$lambda.min, type = "coefficients")
```

```{r}
rr.mod <- glmnet(x_col[train, ], student_por$G3[train], alpha = 0, lambda = cv.out1$lambda.min)
rr.pred <- predict(rr.mod, s = cv.out1$lambda.min, newx = x_col[-train, ])

mean((rr.pred - student_por$G3[-train])^2)
```

## LASSO

```{r}
set.seed(1)
cv.out2 <- cv.glmnet(x_col, student_por$G3, alpha = 1)
predict(cv.out2, s = cv.out2$lambda.min, type = "coefficients")
```

```{r}
lasso.mod <- glmnet(x_col[train, ], student_por$G3[train], alpha = 1, lambda = cv.out2$lambda.min)
lasso.pred <- predict(lasso.mod, s = cv.out2$lambda.min, newx = x_col[-train, ])

mean((lasso.pred - student_por$G3[-train])^2)
```

```{r}
student_por.dimred <- lm(G3 ~ school + sex + reason + failures + G1 + G2, student_por)
summary(student_por.dimred)
```

## Trees

```{r}

```

