---
title: "Final Project"
author: "London Wagner"
date: "4/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r message=FALSE}
# library(here)
library(leaps)
library(tree)
library(glmnet)
library(tidyverse)
```

## Loading in data

```{r message=FALSE}
student_por <- read_csv2("data/student-por.csv")
```


## Introduction

For our final project we are looking at the student performance data set from the UCI Machine Learning Repository that measures student performance in a Portuguese class based on a variety of predictors. There predictors include but are not limited to:

- student sex
- student age
- previous grades
- access to internet, etc

The person who originally collected this data and performed the study was Paulo Cortez from the University of Minho. 

The purpose of our analysis is determine which variables are significant when predicting a student's final grade in the Portuguese class.

Here is our plan for analyzing the data

1) Run a linear regression model with final grade as the response and all other variables as predictors 
2) Use best subset, forward step, and backwards step to select variables for a reduced model
3) Use ridge and lasso
4) Use cross validation methods to determine which model predicts final grade with the greatest accuracy
5) Create a tree using the variables from the best model

## Cleaning Data

```{r}
student_por <-
  student_por %>%
  mutate(school = factor(school),
         sex = factor(sex),
         address = factor(address),
         famsize = factor(famsize),
         Pstatus = factor(Pstatus),
         schoolsup = factor(schoolsup),
         famsup = factor(famsup),
         paid = factor(paid),
         activities = factor(activities),
         nursery = factor(nursery),
         higher = factor(higher),
         internet = factor(internet),
         romantic = factor(romantic))

student_por
```

## Running a simple linear regression

```{r}
por_reg <- lm(G3 ~ ., data = student_por)
summary(por_reg)
```

A lot of the predictors do not appear to be significant, so we are going to use some variable selection methods to simplify the model while still maintaining accuracy.

## Best Subset

```{r}
subsets <- regsubsets(G3 ~ ., data = student_por, nvmax = 15)
summary(subsets)
summary(subsets)$adjr2
summary(subsets)$cp
summary(subsets)$bic
```

## Forward Step

```{r}
null <- lm(G3 ~ 1, data = student_por)
full <- lm(G3 ~ ., data = student_por)

forward <- step(null, scope = list(lower=null, upper = full), direction = "forward")

summary(forward)
```

```{r}
backward <- step(full, scope = list(lower=null, upper = full), direction = "backward")

summary(backward)
```

## Ridge Regression & LASSO Preparation

```{r}
# Training/test split
set.seed(1)
train <- sample(1:nrow(student_por), .7*nrow(student_por))
G3_test <- student_por$G3[-train]

# Creating model matrix for rr and lasso calculations
x_col <- model.matrix(G3 ~ ., student_por)[, -1]
```

## Ridge Regression

```{r}
cv.out1 <- cv.glmnet(x_col, student_por$G3, alpha = 0)
predict(cv.out1, s = cv.out1$lambda.min, type = "coefficients")
```

```{r}
rr.mod <- glmnet(x_col[train, ], student_por$G3[train], alpha = 0, lambda = cv.out1$lambda.min)
rr.pred <- predict(rr.mod, s = cv.out1$lambda.min, newx = x_col[-train, ])

mean((rr.pred - student_por$G3[-train])^2)
```

## LASSO

```{r}
cv.out2 <- cv.glmnet(x_col, student_por$G3, alpha = 1)
predict(cv.out2, s = cv.out2$lambda.min, type = "coefficients")
```

```{r}
lasso.mod <- glmnet(x_col[train, ], student_por$G3[train], alpha = 1, lambda = cv.out2$lambda.min)
lasso.pred <- predict(lasso.mod, s = cv.out2$lambda.min, newx = x_col[-train, ])

mean((lasso.pred - student_por$G3[-train])^2)
```

## Trees

```{r}

```

